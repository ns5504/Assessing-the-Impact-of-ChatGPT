{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ns5504/Assessing-the-Impact-of-ChatGPT/blob/main/ChatGPT_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184ad6ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "184ad6ac",
        "outputId": "4af2654f-b0ba-4d06-bcef-48b18a4a0279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: nlpaug==1.1.11 in /usr/local/lib/python3.11/dist-packages (1.1.11)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (2.32.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug==1.1.11) (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (3.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug==1.1.11) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug==1.1.11) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug==1.1.11) (4.13.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug==1.1.11) (1.7.1)\n",
            "Requirement already satisfied: imbalanced-learn==0.11.0 in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.11.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.11.0) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.11.0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.11.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.11.0) (3.6.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.4.26)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "5dab12dc81114fb887c91516747f704c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install\n",
        "%pip install nlpaug==1.1.11\n",
        "%pip install imbalanced-learn==0.11.0\n",
        "%pip install openpyxl\n",
        "%pip install wordcloud\n",
        "%pip install contractions\n",
        "%pip install vaderSentiment\n",
        "%pip install transformers\n",
        "%pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb13ee1b",
      "metadata": {
        "id": "cb13ee1b"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import glob\n",
        "import pathlib\n",
        "from datetime import datetime\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import (\n",
        "    RobertaTokenizer, RobertaForSequenceClassification,\n",
        "    BertTokenizer, BertForSequenceClassification,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import contractions\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Web requests\n",
        "import requests\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e882c27",
      "metadata": {
        "id": "9e882c27"
      },
      "outputs": [],
      "source": [
        "#Bert Model\n",
        "\n",
        "\n",
        "# =======================\n",
        "# 1. Load and Preprocess Data\n",
        "# =======================\n",
        "df = pd.read_csv(\"/content/Majority Voting 2nd round.csv\")\n",
        "df = df[['Body', 'Majority Vote']].rename(columns={'Body': 'text', 'Majority Vote': 'label'})\n",
        "df = df.dropna()\n",
        "\n",
        "# Get unique labels and their count\n",
        "unique_labels = df['label'].unique()\n",
        "num_labels = len(unique_labels)\n",
        "print(f\"Unique Labels: {unique_labels}, Number of Labels: {num_labels}\")\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])  # Encode labels to numerical values\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# =======================\n",
        "# 2. Tokenization & Dataset Setup\n",
        "# =======================\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "        encoding = self.tokenizer(text, truncation=True, padding='max_length',\n",
        "                                  max_length=self.max_len, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "# =======================\n",
        "# 3. Model and Optimizer\n",
        "# =======================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Update num_labels to match the actual number of labels in your dataset\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# =======================\n",
        "# 4. Training Loop\n",
        "# =======================\n",
        "epochs = 2\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_description(f\"Epoch {epoch+1}\")\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "# =======================\n",
        "# 5. Evaluation\n",
        "# =======================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# =======================\n",
        "# 6. Metrics\n",
        "# =======================\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "report = classification_report(all_labels, all_preds, target_names=label_encoder.classes_)\n",
        "\n",
        "print(f\"\\nâœ… Accuracy: {acc:.4f}\")\n",
        "print(\"ğŸ“Š Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba31183",
      "metadata": {
        "id": "8ba31183"
      },
      "outputs": [],
      "source": [
        "# Roberta model and tokenizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Custom Dataset class remains the same\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Ensure text is a string\n",
        "        text = str(self.texts.iloc[idx])  # Convert to string explicitly\n",
        "        label = self.labels.iloc[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Remove stopwords\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Trainer remains the same\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "# Load the datasets\n",
        "ground_truth_path = '/content/Majority Voting 2nd round.csv'\n",
        "new_data_path = '/content/Academic_data_cleaned.csv'\n",
        "\n",
        "ground_truth = pd.read_csv(ground_truth_path, encoding='latin1')\n",
        "new_data = pd.read_csv(new_data_path)\n",
        "\n",
        "#ground_truth['Body'] = (ground_truth['Title'] + ' ' + ground_truth['Body']).apply(preprocess_text)\n",
        "ground_truth['Body'] = ground_truth['Body'].astype(str)\n",
        "new_data['selftext'] = new_data['selftext'].astype(str)\n",
        "\n",
        "\n",
        "X = ground_truth['Body'].apply(preprocess_text)\n",
        "y = ground_truth['Majority Vote']\n",
        "\n",
        "# Map sentiments to numeric labels\n",
        "label_map = {label: idx for idx, label in enumerate(y.unique())}\n",
        "y = y.map(label_map)\n",
        "\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_map))\n",
        "\n",
        "# Stratified train-test split\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "train_dataset = TextDataset(X, y, tokenizer, max_length=256)\n",
        "test_dataset = TextDataset(new_data['selftext'].apply(preprocess_text), pd.Series([0] * len(new_data)), tokenizer, max_length=256)\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "\"\"\"train_dataset = TextDataset(X_train, y_train, tokenizer, max_length=256)  # Increased max_length\n",
        "val_dataset = TextDataset(X_val, y_val, tokenizer, max_length=256)\"\"\"\n",
        "\n",
        "# Training arguments remain the same\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,  # Adjust learning rate\n",
        "    per_device_train_batch_size=8,  # Adjust batch size\n",
        "    per_device_eval_batch_size=8,  # Adjust batch size\n",
        "    num_train_epochs=10,  # Adjust number of epochs\n",
        "    weight_decay=0.005,  # Adjust weight decay\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
        "                               'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
        "                               'labels': torch.stack([f['labels'] for f in data])},\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Get predictions on the test set\n",
        "predictions = trainer.predict(test_dataset)\n",
        "pred_labels = predictions.predictions.argmax(-1)\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report([0] * len(new_data), pred_labels))\n",
        "accuracy = accuracy_score([0] * len(new_data), pred_labels)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Validation and new dataset predictions remain the same\n",
        "new_data['text'] = (new_data['title'].astype(str) + ' ' + new_data['selftext'].astype(str)).apply(preprocess_text)\n",
        "new_texts = new_data['text']\n",
        "\n",
        "new_encodings = tokenizer(\n",
        "    new_texts.tolist(),\n",
        "    max_length=128,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=new_encodings['input_ids'],\n",
        "        attention_mask=new_encodings['attention_mask']\n",
        "    )\n",
        "    predictions = torch.argmax(outputs.logits, dim=1).numpy()\n",
        "\n",
        "inverse_label_map = {idx: label for label, idx in label_map.items()}\n",
        "new_data['predicted_sentiment'] = [inverse_label_map[pred] for pred in predictions]\n",
        "\n",
        "output_path = '/content/roBerta_model.csv'\n",
        "new_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Labeled dataset saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63761c48",
      "metadata": {
        "id": "63761c48"
      },
      "outputs": [],
      "source": [
        "#Logistic Regression Model\n",
        "\n",
        "# Load the Reddit ground truth dataset\n",
        "ground_truth_path = '/content/drive/MyDrive/Validation sets/majority_voting_results.csv'\n",
        "new_data_path = '/content/drive/MyDrive/Reddit_Data/cleaned/Education/merged_output.csv'\n",
        "\n",
        "ground_truth = pd.read_csv(ground_truth_path, encoding='latin1')\n",
        "new_data = pd.read_csv(new_data_path)\n",
        "\n",
        "# Prepare the training data\n",
        "# Combine 'Title' and 'Body' for better context\n",
        "ground_truth['text'] = ground_truth['Title'] + ' ' + ground_truth['Body']\n",
        "X = ground_truth['text']\n",
        "y = ground_truth['Final Sentimental']\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=120)\n",
        "\n",
        "# Create a pipeline with TfidfVectorizer and LogisticRegression\n",
        "pipeline = make_pipeline(\n",
        "    TfidfVectorizer(max_features=5000),\n",
        "    LogisticRegression(random_state=120)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Validate the model\n",
        "val_predictions = pipeline.predict(X_val)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_val, val_predictions))\n",
        "\n",
        "accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Prepare the new dataset\n",
        "# Combine 'title' and 'selftext' for prediction\n",
        "new_data['text'] = new_data['title'] + ' ' + new_data['selftext']\n",
        "\n",
        "# Predict sentiment for the new data\n",
        "new_data['predicted_sentiment'] = pipeline.predict(new_data['text'])\n",
        "\n",
        "# Save the labeled dataset\n",
        "output_path = '/content/drive/MyDrive/Validation sets/labeled_merged_output2.csv'\n",
        "new_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Labeled dataset saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa2456c5",
      "metadata": {
        "id": "aa2456c5"
      },
      "outputs": [],
      "source": [
        "#Reddit Cleaner\n",
        "\n",
        "def menu_selection():\n",
        "    \"\"\"\n",
        "    Displays the main menu and handles user input for different actions.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        print(\n",
        "            '\\nMenu Options:'\n",
        "            '\\n1. Scrape Reddit data'\n",
        "            '\\n2. Clean Reddit data'\n",
        "            '\\n3. Exit'\n",
        "        )\n",
        "        selection = input('Enter your choice (1/2/3): ').strip()\n",
        "\n",
        "        if selection == '1':\n",
        "            get_information()\n",
        "        elif selection == '2':\n",
        "            path = input('Enter the path to your data file (e.g., /content/yourfile.csv): ').strip()\n",
        "            clean_data(path)\n",
        "        elif selection == '3':\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "        else:\n",
        "            print('Invalid selection. Please try again.')\n",
        "\n",
        "\n",
        "def get_information():\n",
        "    \"\"\"\n",
        "    Collects input from the user to scrape Reddit data and calls the scraper function.\n",
        "    \"\"\"\n",
        "    subreddit = input(\"Enter subreddit(s) to scrape (comma-separated): \").strip()\n",
        "    word = input(\"Enter keyword(s) to search (comma-separated): \").strip()\n",
        "    start = input(\"Enter start date (YYYY-MM-DD): \").strip()\n",
        "    end = input(\"Enter end date (YYYY-MM-DD): \").strip()\n",
        "    folder = input(\"Enter folder to save data (leave blank to save in /content): \").strip()\n",
        "\n",
        "    if not folder:\n",
        "        folder = \"/content\"\n",
        "    else:\n",
        "        folder = os.path.join(\"/content\", folder)\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    parse_tag_list(word, subreddit, start, end, folder)\n",
        "\n",
        "\n",
        "def fetch_reddit_posts(subreddit, start_date, end_date, keyword=None, folder=None):\n",
        "    \"\"\"\n",
        "    Fetches Reddit posts for a specific subreddit and keyword within a date range.\n",
        "    :param subreddit: Subreddit name.\n",
        "    :param start_date: Start date in YYYY-MM-DD format.\n",
        "    :param end_date: End date in YYYY-MM-DD format.\n",
        "    :param keyword: Keyword to search.\n",
        "    :param folder: Folder to save the results.\n",
        "    \"\"\"\n",
        "    start_timestamp = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())\n",
        "    end_timestamp = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp())\n",
        "\n",
        "    base_url = 'https://api.pushshift.io/reddit/search/submission/'\n",
        "\n",
        "    params = {\n",
        "        'subreddit': subreddit,\n",
        "        'after': start_timestamp,\n",
        "        'before': end_timestamp,\n",
        "        'size': 100,\n",
        "        'sort': 'desc',\n",
        "        'sort_type': 'created_utc'\n",
        "    }\n",
        "\n",
        "    if keyword:\n",
        "        params['q'] = keyword\n",
        "\n",
        "    posts = []\n",
        "    while True:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error: Received status code {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "        if 'data' not in data or len(data['data']) == 0:\n",
        "            break\n",
        "\n",
        "        posts.extend(data['data'])\n",
        "        params['before'] = data['data'][-1]['created_utc']\n",
        "\n",
        "    df = pd.DataFrame(posts)\n",
        "    if not df.empty:\n",
        "        df = df[['id', 'title', 'selftext', 'author', 'created_utc', 'url', 'num_comments', 'score']]\n",
        "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
        "\n",
        "        save_path = os.path.join(folder, f\"{subreddit}_{keyword}_{start_date}_{end_date}.csv\")\n",
        "        df.to_csv(save_path, index=False)\n",
        "        print(f\"Data saved to {save_path}\")\n",
        "\n",
        "\n",
        "def parse_tag_list(tag_list, sub_list, start_date, end_date, folder):\n",
        "    \"\"\"\n",
        "    Processes the list of keywords and subreddits, and calls the fetch function for each combination.\n",
        "    \"\"\"\n",
        "    keywords = [tag.strip() for tag in tag_list.split(',')]\n",
        "    subreddits = [sub.strip() for sub in sub_list.split(',')]\n",
        "\n",
        "    for subreddit in subreddits:\n",
        "        for keyword in keywords:\n",
        "            fetch_reddit_posts(subreddit, start_date, end_date, keyword, folder)\n",
        "\n",
        "\n",
        "def clean_data(path):\n",
        "    \"\"\"\n",
        "    Cleans the Reddit data by removing duplicates, empty rows, and posts marked as 'deleted' or 'removed'.\n",
        "    :param path: Path to the CSV file.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Error: The file at '{path}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "        # Remove duplicate rows\n",
        "        df.drop_duplicates(inplace=True)\n",
        "\n",
        "        # Remove rows with NaN values\n",
        "        #df.dropna(inplace=True)\n",
        "\n",
        "        # Remove rows with 'deleted' or 'removed' in 'selftext'\n",
        "        if 'selftext' in df.columns:\n",
        "            df = df[~df['selftext'].str.contains('deleted|removed', case=False, na=False)]\n",
        "\n",
        "        # Save the cleaned data\n",
        "        save_file = path.replace(\".csv\", \"_cleaned.csv\")\n",
        "        df.to_csv(save_file, index=False)\n",
        "        print(f\"Cleaned data saved to {save_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during cleaning: {e}\")\n",
        "\n",
        "\n",
        "# Start the program\n",
        "if __name__ == \"__main__\":\n",
        "    menu_selection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9454312",
      "metadata": {
        "id": "b9454312"
      },
      "outputs": [],
      "source": [
        "# Concatenate CSV files in a directory\n",
        "\n",
        "def concat_csv_in_directory(path, subreddit_domain_file):\n",
        "    # Read the main CSV file; if it's empty, initialize an empty DataFrame with expected columns (if any)\n",
        "    try:\n",
        "        df = pd.read_csv(subreddit_domain_file)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        df = pd.DataFrame()\n",
        "\n",
        "    for file in os.listdir(path):\n",
        "        file_path = os.path.join(path, file)\n",
        "\n",
        "        # Process only CSV files\n",
        "        if os.path.isfile(file_path) and file.endswith(\".csv\"):\n",
        "            try:\n",
        "                df_temp = pd.read_csv(file_path)\n",
        "                df_temp[\"subreddit\"] = file.split('_')[0]  # Add the filename column\n",
        "                df = pd.concat([df, df_temp], ignore_index=True)\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Skipping empty file: {file}\")\n",
        "                continue  # Skip this file if it is empty\n",
        "\n",
        "    df.to_csv(subreddit_domain_file, index=False)\n",
        "    return df\n",
        "\n",
        "def concat_csv(path, subreddit_domain_file):\n",
        "    for file in path.iterdir():\n",
        "        if file.is_file() and file.name.endswith(\".csv\"):\n",
        "            return concat_csv_in_directory(path, subreddit_domain_file)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "file_path = pathlib.Path('/content/Academic_data.csv')\n",
        "folder_path = pathlib.Path('/content/unclean')\n",
        "\n",
        "print(len(os.listdir(folder_path)))\n",
        "\n",
        "if file_path.exists() and file_path.suffix == \".csv\":\n",
        "    print('File found')\n",
        "    df = concat_csv(folder_path, file_path)\n",
        "    print(df)\n",
        "else:\n",
        "    print('File not found')\n",
        "    pd.DataFrame().to_csv(file_path, index=False)  # Create an empty CSV\n",
        "    df = concat_csv(folder_path, file_path)\n",
        "    print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa10ed59",
      "metadata": {
        "id": "fa10ed59"
      },
      "outputs": [],
      "source": [
        "#Reddit Scraper\n",
        "\n",
        "def menu_selection():\n",
        "  print('Input a number from the menu below:'\n",
        "          '\\n1. Scrape Reddit data'\n",
        "          '\\n2. clean Reddit data'\n",
        "          '\\n3. Exit')\n",
        "\n",
        "  selection = input('Enter your choice: ')\n",
        "\n",
        "  if selection == '1':\n",
        "      get_information()\n",
        "\n",
        "  if selection == '2':\n",
        "      path = input('Enter the path to your data: ')\n",
        "      save_path = input('Enter the path to save your data please include .csv: ')\n",
        "      clean_data('./' + path, save_path)\n",
        "\n",
        "  if selection == '3':\n",
        "      return None\n",
        "\n",
        "  elif selection != '1' or selection != '2' or selection != '3':\n",
        "    print('Invalid selection, try again')\n",
        "    menu_selection()\n",
        "\n",
        "def get_information():\n",
        "    subreddit = input(\"Enter the name of the subreddit to scrape, if multiple separate with a comma: \")\n",
        "    word = input(\"Enter the word you'd like to search, if multiple separate each with a comma: \")\n",
        "    start = input(\"Enter the date to begin scraping YYYY-MM-DD: \")\n",
        "    end = input(\"Enter the date to end scraping YYYY-MM-DD: \")\n",
        "    folder = input(\"Enter the folder where you want the data to be saved: \")\n",
        "    parse_tag_list(word, subreddit, start, end, folder)\n",
        "\n",
        "\"\"\"#Scraping function\"\"\"\n",
        "\n",
        "def fetch_reddit_posts(subreddit, start_date, end_date, keyword=None, folder=None):\n",
        "    \"\"\"\n",
        "    Fetches Reddit posts from a given subreddit.\n",
        "    :param folder:\n",
        "    :param subreddit:  The subreddit to fetch posts from.\n",
        "    :param start_date:  The start date to retrieve posts from.\n",
        "    :param end_date:  The end date to retrieve posts from.\n",
        "    :param keyword:  The keyword used to fetch posts from.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert dates to Unix timestamps\n",
        "    start_timestamp = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())\n",
        "    end_timestamp = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp())\n",
        "\n",
        "    base_url = 'https://api.pullpush.io/reddit/search/submission/'\n",
        "\n",
        "    params = {\n",
        "        'subreddit': subreddit,\n",
        "        'after': start_timestamp,\n",
        "        'before': end_timestamp,\n",
        "        'size': 100,  # Number of results per request (max 100)\n",
        "        'sort': 'desc',\n",
        "        'sort_type': 'created_utc'\n",
        "    }\n",
        "\n",
        "    if keyword:\n",
        "        params['q'] = keyword\n",
        "\n",
        "    posts = []\n",
        "    while True:\n",
        "        print(f\"Fetching data with params: {params}\")\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error: Received status code {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "        if 'data' not in data or len(data['data']) == 0:\n",
        "            print(\"No data found or no more posts to fetch.\")\n",
        "            break\n",
        "\n",
        "        posts.extend(data['data'])\n",
        "\n",
        "        print(f\"Fetched {len(data['data'])} posts.\")\n",
        "\n",
        "        # Update the 'before' parameter to the timestamp of the last fetched post\n",
        "        params['before'] = data['data'][-1]['created_utc']\n",
        "\n",
        "    # Create a DataFrame from the posts\n",
        "    df = pd.DataFrame(posts)\n",
        "\n",
        "    # Select relevant columns\n",
        "    if not df.empty:\n",
        "        df = df[['id', 'title', 'selftext', 'author', 'created_utc', 'url', 'num_comments', 'score']]\n",
        "\n",
        "        # Convert the created_utc column to a readable date format\n",
        "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
        "\n",
        "        # Save DataFrame to CSV if a file path is provided\n",
        "        if not folder:\n",
        "          df.to_csv(f\"{subreddit}_{keyword}_{start_date}_{end_date}.csv\", index=False)\n",
        "          print(f\"Saved to {subreddit}_{keyword}_{start_date}_{end_date}.csv\")\n",
        "\n",
        "        if folder and os.path.exists(folder):\n",
        "          print('folder true and path exist')\n",
        "          df.to_csv(f\"{folder}/{subreddit}_{keyword}_{start_date}_{end_date}.csv\", index=False)\n",
        "          print(f\"Saved to {folder}/{subreddit}_{keyword}_{start_date}_{end_date}.csv\")\n",
        "\n",
        "        elif folder and not os.path.exists(folder):\n",
        "          print('folder true but doesnt exist')\n",
        "          try:\n",
        "            os.mkdir(folder)\n",
        "            df.to_csv(f\"{folder}/{subreddit}_{keyword}_{start_date}_{end_date}.csv\", index=False)\n",
        "            print(f\"Saved to {folder}/{subreddit}_{keyword}_{start_date}_{end_date}.csv\")\n",
        "          except:\n",
        "            print('An error occured')\n",
        "    return df\n",
        "\n",
        "\"\"\"#Function that checks for multiple tags and subreddits\"\"\"\n",
        "\n",
        "def parse_tag_list(tag_list, sub_list, start_date, end_date, folder):\n",
        "    keyword_list = tag_list.split(',')\n",
        "    subreddit_list = sub_list.split(',')\n",
        "\n",
        "    for x in range(0, len(subreddit_list)):\n",
        "      subreddit_list[x] = subreddit_list[x].strip()\n",
        "\n",
        "    for x in range(0, len(keyword_list)):\n",
        "      keyword_list[x] = keyword_list[x].strip()\n",
        "\n",
        "    for subreddit in subreddit_list:\n",
        "\n",
        "        for word in keyword_list:\n",
        "            fetch_reddit_posts(subreddit, start_date, end_date, word, folder)\n",
        "\n",
        "\n",
        "\"\"\"#Function to clean data\"\"\"\n",
        "\n",
        "def clean_data(path, save_file):\n",
        "    df = pd.DataFrame({})\n",
        "    # If path is directory concat all data\n",
        "    if os.path.isdir(path):\n",
        "        for file in os.listdir(path):\n",
        "            df_temp = pd.read_csv(os.path.join(path, file))\n",
        "            df = pd.concat([df, df_temp], ignore_index=True)\n",
        "\n",
        "    if not os.path.isdir(path):\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "    # Remove duplicate data points\n",
        "    df.drop_duplicates(keep='first', inplace=True)\n",
        "\n",
        "    # Remove empty data\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Save df to new CSV\n",
        "    df.to_csv(save_file, index=False)\n",
        "\n",
        "\"\"\"#Call function to scrape data\"\"\"\n",
        "\n",
        "menu_selection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c037d176",
      "metadata": {
        "id": "c037d176"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "def remove_stopwords(text) -> str:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = text.split()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "def standardize_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "# Load CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Validation sets/Marcos' Validation dataset - Sheet1.csv\")\n",
        "\n",
        "# download stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(df['Body'])\n",
        "\n",
        "contracted_string = df['Body'].apply(lambda text: contractions.fix(text))\n",
        "#print(contracted_string)\n",
        "\n",
        "lemmatized_string = contracted_string.apply(lemmatize_text)\n",
        "#print(lemmatized_string)\n",
        "\n",
        "df['Body'] = lemmatized_string.apply(standardize_text)\n",
        "\n",
        "no_stpwords_string = df['Body'].apply(remove_stopwords)\n",
        "#print(no_stpwords_string)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(no_stpwords_string)\n",
        "print(tfidf_vectors.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f191786",
      "metadata": {
        "id": "8f191786"
      },
      "outputs": [],
      "source": [
        "#VADER Model\n",
        "\n",
        "# ======================\n",
        "# 1. Load and Prepare Data\n",
        "# ======================\n",
        "df = pd.read_csv(\"Majority Voting 2nd round.csv\")\n",
        "df = df[['Body', 'Majority Vote']].rename(columns={'Body': 'text', 'Majority Vote': 'label'})\n",
        "df = df.dropna()\n",
        "\n",
        "# Encode true labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])  # 0 = Negative, 1 = Positive\n",
        "\n",
        "# ======================\n",
        "# 2. Initialize VADER\n",
        "# ======================\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_vader_label(text):\n",
        "    score = analyzer.polarity_scores(text)[\"compound\"]\n",
        "    if score >= 0.8:  # Relaxed positive threshold\n",
        "        return \"Positive\"\n",
        "    elif score <= -0.8:  # Relaxed negative threshold\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# ======================\n",
        "# 3. Apply VADER to Dataset\n",
        "# ======================\n",
        "df['vader_pred'] = df['text'].apply(get_vader_label)\n",
        "\n",
        "# ======================\n",
        "# 4. Evaluation\n",
        "# ======================\n",
        "accuracy = accuracy_score(df['label_encoded'], df['vader_pred'])\n",
        "report = classification_report(df['label_encoded'], df['vader_pred'], target_names=label_encoder.classes_)\n",
        "\n",
        "print(f\"âœ… VADER Accuracy: {accuracy:.4f}\")\n",
        "print(\"ğŸ“Š VADER Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271ded58",
      "metadata": {
        "id": "271ded58"
      },
      "outputs": [],
      "source": [
        "#Sentiment separation\n",
        "\n",
        "# Ask user for the Excel file path\n",
        "file_path = input(\"Enter the full path to your Excel file (e.g., /content/sentiment.xlsx): \")\n",
        "\n",
        "# Read the Excel file\n",
        "try:\n",
        "    df = pd.read_excel(file_path)\n",
        "except Exception as e:\n",
        "    raise FileNotFoundError(f\"Could not read the file: {e}\")\n",
        "\n",
        "# Ensure required columns exist\n",
        "if not {'Sentiment', 'Body'}.issubset(df.columns):\n",
        "    raise ValueError(\"The Excel file must have 'Sentiment' and 'Body' columns.\")\n",
        "\n",
        "# Normalize sentiment to lowercase\n",
        "df['Sentiment'] = df['Sentiment\n",
        "\n",
        "'].str.lower()\n",
        "\n",
        "# Filter by sentiment\n",
        "positive_df = df[df['Sentiment'] == 'positive'][['Body', 'Sentiment']]\n",
        "negative_df = df[df['Sentiment'] == 'negative'][['Body', 'Sentiment']]\n",
        "neutral_df  = df[df['Sentiment'] == 'neutral'][['Body', 'Sentiment']]\n",
        "\n",
        "# Save to separate Excel files\n",
        "positive_path = 'positive_sentiments.xlsx'\n",
        "negative_path = 'negative_sentiments.xlsx'\n",
        "neutral_path  = 'neutral_sentiments.xlsx'\n",
        "\n",
        "positive_df.to_excel(positive_path, index=False)\n",
        "negative_df.to_excel(negative_path, index=False)\n",
        "neutral_df.to_excel(neutral_path, index=False)\n",
        "\n",
        "print(\"\\nâœ… Files saved successfully:\")\n",
        "print(f\"- {positive_path}\")\n",
        "print(f\"- {negative_path}\")\n",
        "print(f\"- {neutral_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffc90f00",
      "metadata": {
        "id": "ffc90f00"
      },
      "outputs": [],
      "source": [
        "# Agreement Score Calculation\n",
        "\n",
        "def calculate_agreement_score():\n",
        "    # Prompt the user to enter the file paths for each file\n",
        "    majority_file = input(\"Enter the path of the Majority Voting result file: \")\n",
        "    comparison_file = input(\"Enter the path of the comparison file: \")\n",
        "\n",
        "    try:\n",
        "        # Load the two Excel files\n",
        "        majority_df = pd.read_excel(majority_file)\n",
        "        comparison_df = pd.read_excel(comparison_file)\n",
        "\n",
        "        # Check if the 'Majority Vote' column exists in the majority file\n",
        "        if 'Majority Vote' not in majority_df.columns:\n",
        "            print(\"Error: 'Majority Vote' column not found in the majority voting file.\")\n",
        "            return None\n",
        "\n",
        "        # Check if the 'Sentiment' column exists in the comparison file\n",
        "        if 'Sentiment' not in comparison_df.columns:\n",
        "            print(\"Error: 'new Sentiment' column not found in the comparison file.\")\n",
        "            return None\n",
        "\n",
        "        # Make sure both files have the same number of rows for comparison\n",
        "        if len(majority_df) != len(comparison_df):\n",
        "            print(\"Error: Files do not have the same number of rows.\")\n",
        "            return None\n",
        "\n",
        "        # Capitalize the values in both columns for consistency\n",
        "        majority_df['Majority Vote'] = majority_df['Majority Vote'].str.capitalize()\n",
        "        comparison_df['Sentiment'] = comparison_df['Sentiment'].str.capitalize()\n",
        "\n",
        "        # Calculate the agreement (count how many rows are the same in both columns)\n",
        "        agreements = (majority_df['Majority Vote'] == comparison_df['Sentiment']).sum()\n",
        "        total = len(majority_df)\n",
        "        agreement_score = (agreements / total) * 100\n",
        "\n",
        "        # Output the agreement score\n",
        "        print(f\"\\nAgreement Score: {agreement_score:.2f}%\")\n",
        "        return agreement_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Run the function to calculate the agreement score\n",
        "calculate_agreement_score()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b61dfa5",
      "metadata": {
        "id": "5b61dfa5"
      },
      "outputs": [],
      "source": [
        "# Word Cloud\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Reddit_Data/GradSchool_chatgpt_2023-1-1_2023-4-30.csv\"  # Adjust this to point to the 'reddit' folder on your flash drive\n",
        "file_pattern = os.path.join(folder_path, \"*.csv\")\n",
        "files = glob.glob(file_pattern)\n",
        "\n",
        "# Set the stopwords list\n",
        "stopwords = set(STOPWORDS)\n",
        "new_words = [ 'https','i', 'you', 'me', 'a', 'us','thank','you','chatgpt','ve','lot','please','now','something']  # Add other words you want to exclude\n",
        "new_stopwords = stopwords.union(new_words)\n",
        "\n",
        "# df = pd.read_csv(files)\n",
        "if files:\n",
        "    # Read the first (and presumably only) CSV file\n",
        "    df = pd.read_csv(files)\n",
        "else:\n",
        "    print(\"No CSV files found in the specified folder.\")\n",
        "    # Handle the case where no files are found, e.g., exit or raise an exception\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
        "\n",
        "if text.strip():  # Check if text is not empty or just whitespace\n",
        "    wordcloud = WordCloud(\n",
        "        max_font_size=50,\n",
        "        max_words=100,\n",
        "        background_color=\"white\",\n",
        "        stopwords=new_stopwords,\n",
        "        colormap='Dark2'  # Choose a contrasting colormap\n",
        "    ).generate(text)\n",
        "\n",
        "    # Plot Wordcloud\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No text available to generate word cloud.\")\n",
        "\n",
        "if text.strip():  # Check if text is not empty or just whitespace\n",
        "    wordcloud = WordCloud(\n",
        "        max_font_size=50,\n",
        "        max_words=100,\n",
        "        background_color=\"white\",\n",
        "        stopwords=new_stopwords,\n",
        "        colormap='Dark2'  # Choose a contrasting colormap\n",
        "    ).generate(text)\n",
        "\n",
        "    # Plot Wordcloud\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No text available to generate word cloud.\")\n",
        "\n",
        "# Size of Word Cloud\n",
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "\n",
        "# Make Wordcloud\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\",stopwords=new_stopwords, colormap='flag').generate(text)\n",
        "\n",
        "# Plot Wordcloud\n",
        "plt.plot()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}